\section{On-demand open-world evaluation}
\label{sec:method}

Pooling bias is fundamentally a sampling bias problem where relational tuples from pooled systems are overrepresented and those from new systems are underrepresented in the evaluation dataset.
We could of course sidestep the problem by exhaustively annotating the entire document corpus, but that would be a laborious and prohibitively expensive task:
using the interfaces we've developed (described in detail in \refsec{evaluation}), it costs about \$15 to annotate a single document by non-expert crowdworkers, leading to an estimated cost of at least \$1,350,000 for a reasonably large corpus of 90,000 documents \cite{}.
The annotation effort would cost significantly more with expert annotators.
% TODO: highlight contrast with pooling.
In contrast, annotating data under the pooling methodology can be significantly cheaper because it is much simpler to \textit{verify} relations predicted by a system than to find it in a document: for our interfaces, it costs only about \$0.15 per relation to verify a predicted relational tuple compared to \$1.6 per relation via exhaustive annotation. 

We propose a new paradigm called on-demand open-world evaluation which takes a lazy approach to dataset construction by annotating predictions from systems \textit{only when they are underrepresented}, thus correcting for pooling bias as it arises.
In this section, we'll formalize the problem solved by on-demand open-world evaluation independent of KBP and describe our solution that allows us to accurately estimate evaluation metrics without bias in a cost-effective manner. 
We'll then instantiate the framework for KBP in \refsec{application}.

\subsection{Problem statement}
Let $\sX$ be a universe of possible system outputs (e.g.\ relational tuples),
  $\sY \subseteq \sX$ be an unknown subset of this universe corresponding to the correct elements in $\sX$,
  $X_1, \ldots X_m \subseteq \sX$ be known subsets that correspond to the predicted output from $m$ systems,
  and let $Y_1, \ldots, Y_m$ be the intersection of $X_1, \ldots, X_m$ with $\sY$.
Formally, let $f(x) \eqdef \I[x \in \sY]$ and $g_i(x) = \I[x \in X_i]$, then the precision, $\pi_i$, and recall, $\rho_i$, of the set of predictions $X_i$ is
\begin{align*}
  \pi_i  &\eqdef \E_{x \sim X_i}[f(x)] &
  \rho_i &\eqdef \E_{x \sim \sY}[g_i(x)],
\end{align*}
where $x$ is sampled from $X_i$ and $\sY$ according to distributions $p_i(x)$ and $p'(x)$ respectively.
We assume that $p_i(x)$ is known, e.g.\ the uniform distribution over $X_i$, and that samples from $p'(x)$ can be obtained, even if it is unknown.

In on-demand open-world evaluation, we are allowed to ask if $x \in \sY$ (e.g.\ by assessing a system's prediction) or for samples from $\sY$ (e.g.\ by exhaustively annotating a document) at a certain (monetary) cost.
Typically, checking if $x \in \sY$ is significantly cheaper than asking for samples from $\sY$.
We only see the sets $X_1, \ldots, X_m$ sequentially, as and when they are submitted to the evaluation framework, and
  our goal is to estimate $\pi_i$ and $\rho_i$ within a confidence interval of $\epsilon$ at a minimal cost. 

Clearly, each $\pi_i$ and $\rho_i$ can both be estimated by sampling from $X_i$ and $\sY$ respectively.
Let $\Xh_1, \ldots, \Xh_m$ be $n_1, \ldots, n_j$ samples from $X_1, \ldots, X_m$ respectively, and let $\Yh_0$ be a set of $n_0$ samples drawn from from $\sY$.
Then, a simple estimator for precision and recall is:
\begin{align*}
  \pih_i^{(s)} &= \sum_{x \in \Xh_i} f(x) & \rhoh_i^{(s)} &= \sum_{x \in \Yh_0} g_i(x).
\end{align*}
%However, simple statistics tell us that we could require at least 10,000 independent samples each to estimate $\pi$ and $\rho$ to $\pm 1\%$, which would be quite costly on a per-system basis.
To make the system practically viable though, we'd like be able to reuse the samples we've collected and only spend money to annotate data when absolutely necessary.
In the rest of this section, we'll see how to do this by answering the following three key questions:
\begin{enumerate}
  \item How can we use all the samples $\Xh_1, \ldots \Xh_m$ when estimating $\pi_i$?
  \item Can we use the samples $\Xh_1, \ldots, \Xh_m$ when estimating $\rho_i$ in conjunction with $\Yh_0$?
  \item Finally, how many samples should we draw from $X_m$, given existing samples $\Yh_0$ and $\Xh_1, \ldots, \Xh_{m-1}$?
\end{enumerate}
The whole approach is summarized in \refalg{on-demand-sampling}.

\begin{algorithm}
  \caption{\label{alg:on-demand-sampling} The on-demand open-world evaluation methodology}
\begin{algorithmic}
  \REQUIRE{
  A sequence of predicted output sets $X_1, \ldots X_m \subseteq \sX$,
  a method of evaluating $f(x) = \I[x \in \sY]$ and a method of sampling $x \sim \sY$,
  desired confidence intervals $\epsilon$.
  }
  \ENSURE{Unbiased predictions of precision, $\pih_1, \ldots, \pih_m$ and recall $\rhoh_1, \ldots \rhoh_m$.}

  % Collect a dataset Y_0
  \STATE{Sample a set $\Yh_0$ from $\sY$ based on $\epsilon_\rho$.}
  % For each sample X_i, 
  \FOR{$i = 0$ \TO{} $i = m$}
  % compute the optimal number of samples $n_i$ required to estimate $\pih_i$ to within $\epsilon$ given $\Xh_{1} - \Xh_{i-1}.
  \STATE{Compute the minimum number of samples $n_i$ required to estimate $\pi_i$ within $\epsilon$.}
  % Draw $n_i$ samples from $X_i$ and evaluate $f(x)$ on these samples.
  \STATE{Draw $n_i$ samples from $X_i$ to form $\Xh_i$.}
  \STATE{Evaluate $f(x)$ on $\Xh_i$.}

  \FOR{$j = 0$ \TO{} $j = i$}
    % For j,
    \STATE{Use $\Xh_1, \ldots \Xh_i$ and $\Yh_0$ to update estimates for $\pi_j$ and $\rho_j$.}
  \ENDFOR{}
  \ENDFOR{}
\end{algorithmic}
\end{algorithm}

\subsection{Estimating precision}

Intuitively, if a set $X_j$ has a significant overlap with $X_i$, we expect that we should be able to its samples when estimating $\pi_i$.
However, it might be case that $X_j$ overlaps with $X_i$ only when $p_i(x)$ is relatively small, in which case the sample $\Xh_j$ is not representative of $X_i$ and a naive combination could lead to the wrong estimate of $\pi_i$.
We address this problem by using importance sampling \citep{owen2013monte}.

In particular, the estimator that we propose is:
\begin{align*}
  \pih_i &= \sum_{j=1}^m \frac{w_{ij}}{n_{j}} \sum_{x \in \Xh_j \intersection X_i} \frac{p_i(x) f(x)}{q_i(x)},
\end{align*}
where $q_i(x) = \sum_{j=1}^m w_{ij} p_j(x)$ and $w_{ij} \ge 0$ are mixture parameters such that $\sum_{j=1}^m w_{ij} = 1$ and $q_i(x) > 0$ wherever $p_i(x) > 0$.
This last condition is easy to guarantee by setting $w_{ii} > 0$.
Intuitively, $q_i(x)$ behaves like the marginal distribution over $\sX$ if we drew samples the mixture of $p_j(x)$, with mixture weights $w_j$.

In \appendixref{sampling} we prove that $\pih_i$ is an unbiased estimator of $\pi_i$ and also work out an expression for its variance. 
The variance of course depends on $f(x)$, but the general intuition is that 
$\pih_i$ will have high variance if $q_i(x) \ll p_i(x)$.
In particular, the ideal choice of $w_{ij}$ for a system $j$ with disjoint predictions to system $i$ is $0$, and if $X_i$ and $X_j$ are identical, $w_{ij} \propto n_{j}$.
This motivates the choice $w_{ij} \propto n_{j} \sum_{x \in \sX} p_j(x) p_i(x)$.

\subsection{Estimating recall}
Let $\Yh_1, \ldots, \Yh_m$ be the intersection of $\Xh_i$ with $\sY$.
While $\Yh_i$ form a randomly sampled subset of $\sY$ that we could evaluate recall on, it is a very biased subset.
To see this, consider the case where the pool $\Yh$ is formed from a single system; here that system would be given a perfect recall score, irrespective of how good it is.
The magnitude of this bias may decrease with more and more systems, but it is unwise to assume that the pool of systems sufficiently covers all the elements of $\sY$.\footnote{
  In particular, for the task of KBP, the recall of systems is bottlenecked by the NER systems they use, etc.
}
We correct for the bias by adjusting the recall of a system measured against the pool $Y$ (the \textit{pooled recall}), $\nu_i = \E_{x\sim Y}[g_i(x)]$, by multiplying it by the recall of the pool itself, $\theta = \E_{x\sim\sY}[\I[x \in Y]]$: $\rho_i = \theta \nu_i$.
Our proposed estimator estimator for recall $\rho_i$ is then $\rhoh_i \eqdef \thetah\nuh_i$, where $\thetah$ is $\sum_{x \in \Yh_0} \I[x \in \Union_{i=1}^m X_i]$ and $\nuh_i$ is a self-normalized importance-weighted estimator:
\begin{align*}
  \nuh_i &\eqdef \frac{\sum_{j=1}^m \frac{w_j}{n_j} \sum_{x \in \Yh_j} \frac{u(x) g_i(x)}{q(x)}}{\sum_{j=1}^m \frac{w_j}{n_j} \sum_{x \in \Yh_j} \frac{u(x)}{q(x)}},
\end{align*}
where $p'(x) \propto u(x)$, $q(x) = \sum_{j=1}^m w_{j} p_j(x)$ and $w_{j} \ge 0$ are mixture parameters such that $\sum_{j=1}^m w_{j} = 1$. We use $w_j \propto n_j$.
In \appendixref{sampling}, we show that $\rhoh_i$ is unbiased.

\subsection{Adaptively drawing samples for new sets}
Finally, the desired property for our framework is to annotate new data only when necessary, i.e.\ a new submission $X_m$ contains sufficiently diverse output.
We formalize this statement by requesting for a target variance $\epsilon$ for our precision estimates.
The variance of $\pih_m$ is a complex non-convex function, but we know that it is monotonically decreasing in $n_m$, the number of samples drawn from the new output, $X_m$.
Consequently it is quite easy to solve for the number of samples needed to achieve a target variance of $\epsilon$ using a bisection method.
