\section{On-demand evaluation with importance sampling}
\label{sec:method}

Pooling bias is fundamentally a sampling bias problem where relation instances from new systems are underrepresented in the evaluation dataset.
We could of course sidestep the problem by exhaustively annotating the entire document corpus, but that would be a laborious and prohibitively expensive task:
using the interfaces we've developed (\refsec{evaluation}), it costs about \$15 to annotate a single document by non-expert crowdworkers, resulting in an estimated cost of at least \$1,350,000 for a reasonably large corpus of 90,000 documents \citep{XXX}.
The annotation effort would cost significantly more with expert annotators.
% TODO: highlight contrast with pooling.
In contrast, \textit{verifying} relation instances pooled from system predictions \pl{can we remove 'pooled from system predictions'?} can be an order of magnitude cheaper than finding them in documents: using our interfaces, it costs only about \$0.18 to verify each pooled instances compared to \$1.60 per instance extracted through exhaustive annotations.
\pl{why the diff between 0.18 and 1.60?  Isn't it the same problem of labeling an instance?}

We propose a new paradigm called on-demand evaluation which takes a lazy approach to dataset construction by annotating predictions from systems \textit{only when they are underrepresented}, thus correcting for pooling bias as it arises.
In this section, we'll formalize the problem solved by on-demand evaluation independent of KBP and describe a cost-effective solution that allows us to accurately estimate evaluation metrics \pl{be consistent: scores} without bias using importance sampling.
We'll then instantiate the framework for KBP in \refsec{application}.

\subsection{Problem statement}
Let $\sX$ be a universe of candidate predictions (e.g.\, relation instances),
  $\sY \subseteq \sX$ be the unknown subset of correct instances,
  $X_1, \ldots X_m \subseteq \sX$ be the predictions for $m$ systems,
  and let $Y_1, \ldots, Y_m$ be the intersection of $X_1, \ldots, X_m$ with $\sY$.
Let $X = \Union_{i=1}^m X_i$ and $Y = \Union_{i=1}^m Y_i$.
Let $f(x) \eqdef \I[x \in \sY]$ and $g_i(x) = \I[x \in X_i]$, then the precision, $\pi_i$, and recall, $\rho_i$, of the set of predictions $X_i$ is
\begin{align*}
  \pi_i  &\eqdef \E_{x \sim X_i}[f(x)] &
  \rho_i &\eqdef \E_{x \sim \sY}[g_i(x)],
\end{align*}
where $x$ is sampled from $X_i$ and $\sY$ according to distributions $p_i(x)$ and $p'(x)$ respectively.
We assume that $p_i(x)$ is known, e.g.\, the uniform distribution over $X_i$
  and that we know $p'(x)$ up to normalization constant, i.e.\ we know $p'(x) \propto u(x)$.
Finally, we assume that we are able to draw samples uniformly from $\sY$.

In on-demand evaluation, we are allowed to ask if $x \in \sY$ (e.g.\, by verifying a prediction) or for samples from $\sY$ (e.g.\, by exhaustively annotating a document) at a certain (monetary) cost.
Typically, checking if $x \in \sY$ is significantly cheaper than asking for samples from $\sY$.
We only see the sets $X_1, \ldots, X_m$ sequentially, as and when they are submitted for evaluation, and
  our goal is to estimate $\pi_i$ and $\rho_i$ within a confidence interval $\epsilon$ at a minimal cost. 

\subsection{Simple estimator}
Both $\pi_i$ and $\rho_i$ can be estimated by sampling from $X_i$ and $\sY$ respectively.
Let $\Xh_1, \ldots, \Xh_m$ be $n_1, \ldots, n_j$ samples from $X_1, \ldots, X_m$ respectively, and let $\Yh_0$ be a set of $n_0$ samples drawn from $\sY$.
Then, a simple baseline estimator for precision and recall is:
\begin{align}
  \pih_i^{(s)} &= \sum_{x \in \Xh_i} f(x) & \rhoh_i^{(s)} &= \sum_{x \in \Yh_0} g_i(x).
\end{align}

\subsection{Joint estimator}
The simple estimator leaves much wanting: in particular, because it does not use samples collected from other systems, it will have a larger variance and requires us to spend money to collect data for every new system submitted.
We'll address these problems by answering the following three key questions:
\begin{enumerate}
  \item How can we use all the samples $\Xh_1, \ldots \Xh_m$ when estimating $\pi_i$?
  \item How can we combine all the samples $\Xh_1, \ldots, \Xh_m$ with $\Yh_0$ when estimating $\rho_i$?
  \item Finally, how many samples should we draw from $X_m$, given existing samples $\Yh_0$ and $\Xh_1, \ldots, \Xh_{m-1}$?
\end{enumerate}
%The whole approach is summarized in \refalg{on-demand-sampling}.
%
%\begin{algorithm}
%  \caption{\label{alg:on-demand-sampling} The on-demand evaluation methodology}
%\begin{algorithmic}
%  \REQUIRE{
%  A sequence of predicted output sets $X_1, \ldots X_m \subseteq \sX$,
%  a method of evaluating $f(x) = \I[x \in \sY]$ and a method of sampling $x \sim \sY$,
%  desired confidence intervals $\epsilon$.
%  }
%  \ENSURE{Unbiased predictions of precision, $\pih_1, \ldots, \pih_m$ and recall $\rhoh_1, \ldots \rhoh_m$.}
%
%  % Collect a dataset Y_0
%  \STATE{Sample a set $\Yh_0$ from $\sY$ based on $\epsilon_\rho$.}
%  % For each sample X_i, 
%  \FOR{$i = 0$ \TO{} $i = m$}
%  % compute the optimal number of samples $n_i$ required to estimate $\pih_i$ to within $\epsilon$ given $\Xh_{1} - \Xh_{i-1}.
%  \STATE{Compute the minimum number of samples $n_i$ required to estimate $\pi_i$ within $\epsilon$.}
%  % Draw $n_i$ samples from $X_i$ and evaluate $f(x)$ on these samples.
%  \STATE{Draw $n_i$ samples from $X_i$ to form $\Xh_i$.}
%  \STATE{Evaluate $f(x)$ on $\Xh_i$.}
%
%  \FOR{$j = 0$ \TO{} $j = i$}
%    % For j,
%    \STATE{Use $\Xh_1, \ldots \Xh_i$ and $\Yh_0$ to update estimates for $\pi_j$ and $\rho_j$.}
%  \ENDFOR{}
%  \ENDFOR{}
%\end{algorithmic}
%\end{algorithm}

\paragraph{Estimating precision jointly.}
Intuitively, if two systems have very similar predictions $X_i$ and $X_j$, we should be able to use samples from one to estimate precision on the other.
However, it might also be the case that $X_i$ and $X_j$ only overlap on a small region, in which case the samples from $X_j$ do not accurately represent instances in $X_i$ and could lead to a biased estimate.
We address this problem by using importance sampling \citep{owen2013monte}, a standard statistical technique for estimating properties of one distribution using samples from another distribution.

% ARUN: We should say something like:
\ac{I'm struggling to say the below in some sensible fashion.}
With importance sampling, we draw samples $\Xh$ from $X$ using a proposal distribution $q(x)$ and use these samples to estimate $\pi_i$ for each $p_i(x)$ by reweighting them by a factor of $\frac{p_i(x)}{q(x)}$, i.e. $\pih_i^{(i)} \eqdef \sum_{x \in \Xh} \frac{p_i(x)}{q(x)} f(x)$.
Unfortunately, because $X$ and therefore $q$ changes every time a new set of predictions is submitted, samples drawn earlier are no longer valid.
%At the same time, using a single proposal distribution when estimating different $\pi_i$ could be suboptimal.
% - For example, the proposal distribution could be the weighted mixture over all $X_i$: 
% $q(x) = \frac{1}{m} \sum_{i=1}^m w_i p_i(x)$, for some $w_i > 0, \sum_{i}^m w_i = 1$.
% - However, when a new $X_{m+1}$ is being evaluated, the estimator cannot make use of the samples previously obtained because they would have come from a different proposal distribution.
% - Furthermore, we would prefer reweighting samples when estimated for each $\pi_i$ differently.

% We handle these two desiderata using the following joint precision estimator:
%\ac{Other readers have noted that it might be good to give intuition about this estimator. I'm not sure how to do so given space constraints.}
We overcome this obstacle by proposing the following joint precision estimator that simulates the behavior of a proposal distribution without actually drawing samples from it:
\begin{align}
  \pih_i^{(j)} &= \sum_{j=1}^m \frac{w_{ij}}{n_{j}} \sum_{x \in \Xh_j} \frac{p_i(x) f(x)}{q_i(x)},
\end{align}
where $q_i(x) = \sum_{j=1}^m w_{ij} p_j(x)$ and $w_{ij} \ge 0$ are mixture parameters such that $\sum_{j=1}^m w_{ij} = 1$ and $q_i(x) > 0$ wherever $p_i(x) > 0$.
This last condition is easy to guarantee by setting $w_{ii} > 0$.
Intuitively, $q_i(x)$ behaves like the marginal distribution over $\sX$ if we drew samples the mixture of $p_j(x)$, with mixture weights $w_{ij}$.\footnote{%
We provide rigorous proofs for the unbiasedness of the proposed estimators in \refapp{sampling} of the supplementary material.}

The variance depends on $f(x)$, but the general intuition is that $\pih_i^{(j)}$ will have high variance if $q_i(x) \ll p_i(x)$.
In particular, we can show that the ideal choice of $w_{ij}$ for if $X_i$ and $X_j$ are disjoint is $0$, and if $X_i$ and $X_j$ are identical is $w_{ij} \propto n_{j}$.
This motivates the choice $w_{ij} \propto n_{j} \sum_{x \in \sX} p_j(x) p_i(x)$.

\paragraph{Estimating recall jointly.}
During the process of evaluating $f(x)$ on $\Xh_i$, we also are able to identify $\Yh_i \eqdef \Xh_i \intersection \sY$.
The collection of $\Yh = \Union_{i=1}^m \Yh_i$ forms a random subset of $\sY$ that is much cheaper to obtain than by sampling from $\sY$, but this subset can extremely biased if we were to use it to evaluate recall: 
If the pool $\Yh$ contained just one system, that system would be given a perfect recall score irrespective of how bad it was.
We dub this incomplete estimate pooled recall, $\nu_i \eqdef \E_{x \sim Y}[f(x) g_i(x)]$.
The remaining factor must compensate for how much of $\sY$ the pool $Y$ actually covers, i.e.\ the pool recall: $\theta \eqdef \E_{x \sim \sY}[g(x)]$ where $g(x) \eqdef \I[x \in X]$.
Thus, the recall of a system is simply the product: $\rho_i = \theta \nu_i$.\footnote{%
We present a rigorous proof in \refapp{sampling} of the supplementary material.}

The joint estimator for recall is then $\rhoh_i^{(j)} \eqdef \thetah\nuh_i$, where $\thetah$ is $\sum_{x \in \Yh_0} g(x)$ and $\nuh_i$ is a self-normalized importance-weighted estimator:
\begin{align*}
  \nuh_i &\eqdef \frac{\sum_{j=1}^m \frac{w_{ij}}{n_j} \sum_{x \in \Xh_j} \frac{u(x) g_i(x)}{q_i(x)}}{\sum_{j=1}^m \frac{w_{ij}}{n_j} \sum_{x \in \Xh_j} \frac{u(x)}{q_i(x)}},
\end{align*}
where $q_i$ and $w_{ij}$ are the same as defined for $\pih_i^{(j)}$.

\paragraph{Adaptively choosing the number of samples.}
Finally, a desired property for on-demand evaluation is to annotate new data only when the current evaluation data is unable to do the job, e.g.\ when a new set of predictions $X_m$ contains many instances not covered by other systems.
We can measure how well the current evaluation set covers the predictions $X_m$ by using an upper bound on the variance of $\pih_m^{(j)}$.\footnote{Details can be found in \refapp{sampling} of the supplementary material.}
In particular, the variance of $\pih_m^{(j)}$ is a monotonically decreasing functions in the number of samples of $X_m$ drawn $n_m$ and we can easily solve for the minimum number of samples required to estimate $\pih_m^{(j)}$ within a confidence interval $\epsilon$ by using the bisection method \citep{burden1985bisection}. \ac{I feel like this conspicuously omits $\rho_i$ from the picture, and that's because the variance of $\nu_i$ is god-awfully nasty.}

\paragraph{Summary.}
On-demand evaluation with the joint estimator works as follows:
First $\Yh_0$ is randomly sampled from $\sY$ once when the evaluation framework is launched.
For every new set of predictions $X_m$ submitted for evaluation, the minimum number of samples $n_i$ required to accurately evaluate $X_m$ is calculated based on the current evaluation data, $\Yh_0$ and $\Xh_1, \ldots, \Xh_{m-1}$.
Then, the set $\Xh_m$ is added to the evaluation data by evaluating $f(x)$ on $n_m$ samples drawn from $X_m$.
Finally, estimates $\pi_1, \ldots, \pi_m$ and $\rho_1, \ldots, \rho_m$ are updated using the joint estimators.
