\section{On-demand evaluation with importance sampling}
\label{sec:method}

Pooling bias is fundamentally a sampling bias problem where relation instances from new systems are underrepresented in the evaluation dataset.
We could of course sidestep the problem by exhaustively annotating the entire document corpus, but that would be a laborious and prohibitively expensive task:
using the interfaces we've developed (\refsec{evaluation}), it costs about \$15 to annotate a single document by non-expert crowdworkers, resulting in an estimated cost of at least \$1,350,000 for a reasonably large corpus of 90,000 documents \citep{}.
The annotation effort would cost significantly more with expert annotators.
% TODO: highlight contrast with pooling.
In contrast, \textit{verifying} relation instances pooled from system predictions can be an order of magnitude cheaper than finding them in documents: using our interfaces, it costs only about \$0.18 per instance to verify pooled instances compared to \$1.60 per instance extracted through exhaustive annotations. 

We propose a new paradigm called on-demand evaluation which takes a lazy approach to dataset construction by annotating predictions from systems \textit{only when they are underrepresented}, thus correcting for pooling bias as it arises.
In this section, we'll formalize the problem solved by on-demand evaluation independent of KBP and describe a cost-effective solution that allows us to accurately estimate evaluation metrics without bias using importance sampling.
We'll then instantiate the framework for KBP in \refsec{application}.

\subsection{Problem statement}
Let $\sX$ be a universe of candidate predictions (e.g.\, relation instances),
  $\sY \subseteq \sX$ be the unknown subset of correct instances,
  $X_1, \ldots X_m \subseteq \sX$ be the predictions for $m$ systems,
  and let $Y_1, \ldots, Y_m$ be the intersection of $X_1, \ldots, X_m$ with $\sY$.
Let $f(x) \eqdef \I[x \in \sY]$ and $g_i(x) = \I[x \in X_i]$, then the precision, $\pi_i$, and recall, $\rho_i$, of the set of predictions $X_i$ is
\begin{align*}
  \pi_i  &\eqdef \E_{x \sim X_i}[f(x)] &
  \rho_i &\eqdef \E_{x \sim \sY}[g_i(x)],
\end{align*}
where $x$ is sampled from $X_i$ and $\sY$ according to distributions $p_i(x)$ and $p'(x)$ respectively.
We assume that $p_i(x)$ is known, e.g.\, the uniform distribution over $X_i$
  and that we know $p'(x)$ up to normalization constant, i.e.\ we know $p'(x) \propto u(x)$.
Finally, we assume that we are able to draw samples uniformly from $\sY$.

In on-demand evaluation, we are allowed to ask if $x \in \sY$ (e.g.\, by verifying a prediction) or for samples from $\sY$ (e.g.\, by exhaustively annotating a document) at a certain (monetary) cost.
Typically, checking if $x \in \sY$ is significantly cheaper than asking for samples from $\sY$.
We only see the sets $X_1, \ldots, X_m$ sequentially, as and when they are submitted for evaluation, and
  our goal is to estimate $\pi_i$ and $\rho_i$ within a confidence interval $\epsilon$ at a minimal cost. 

\subsection{Simple estimator}
Both $\pi_i$ and $\rho_i$ can be estimated by sampling from $X_i$ and $\sY$ respectively.
Let $\Xh_1, \ldots, \Xh_m$ be $n_1, \ldots, n_j$ samples from $X_1, \ldots, X_m$ respectively, and let $\Yh_0$ be a set of $n_0$ samples drawn from $\sY$.
Then, a simple baseline estimator for precision and recall is:
\begin{align}
  \pih_i^{(s)} &= \sum_{x \in \Xh_i} f(x) & \rhoh_i^{(s)} &= \sum_{x \in \Yh_0} g_i(x).
\end{align}

\subsection{Joint estimator}
The simple estimator leaves much wanting: in particular, because it does not use samples collected from other systems, it will have a larger variance and requires us to spend money to collect data for every new system submitted.
In order to tackle these failings we'll need to answer the following three key questions:
\begin{enumerate}
  \item How can we use all the samples $\Xh_1, \ldots \Xh_m$ when estimating $\pi_i$?
  \item How can we combine all the samples $\Xh_1, \ldots, \Xh_m$ with $\Yh_0$ when estimating $\rho_i$?
  \item Finally, how many samples should we draw from $X_m$, given existing samples $\Yh_0$ and $\Xh_1, \ldots, \Xh_{m-1}$?
\end{enumerate}
%The whole approach is summarized in \refalg{on-demand-sampling}.
%
%\begin{algorithm}
%  \caption{\label{alg:on-demand-sampling} The on-demand evaluation methodology}
%\begin{algorithmic}
%  \REQUIRE{
%  A sequence of predicted output sets $X_1, \ldots X_m \subseteq \sX$,
%  a method of evaluating $f(x) = \I[x \in \sY]$ and a method of sampling $x \sim \sY$,
%  desired confidence intervals $\epsilon$.
%  }
%  \ENSURE{Unbiased predictions of precision, $\pih_1, \ldots, \pih_m$ and recall $\rhoh_1, \ldots \rhoh_m$.}
%
%  % Collect a dataset Y_0
%  \STATE{Sample a set $\Yh_0$ from $\sY$ based on $\epsilon_\rho$.}
%  % For each sample X_i, 
%  \FOR{$i = 0$ \TO{} $i = m$}
%  % compute the optimal number of samples $n_i$ required to estimate $\pih_i$ to within $\epsilon$ given $\Xh_{1} - \Xh_{i-1}.
%  \STATE{Compute the minimum number of samples $n_i$ required to estimate $\pi_i$ within $\epsilon$.}
%  % Draw $n_i$ samples from $X_i$ and evaluate $f(x)$ on these samples.
%  \STATE{Draw $n_i$ samples from $X_i$ to form $\Xh_i$.}
%  \STATE{Evaluate $f(x)$ on $\Xh_i$.}
%
%  \FOR{$j = 0$ \TO{} $j = i$}
%    % For j,
%    \STATE{Use $\Xh_1, \ldots \Xh_i$ and $\Yh_0$ to update estimates for $\pi_j$ and $\rho_j$.}
%  \ENDFOR{}
%  \ENDFOR{}
%\end{algorithmic}
%\end{algorithm}

\paragraph{Estimating precision jointly.}
Intuitively, if two systems have very similar predictions $X_i$ and $X_j$, we should be able to use samples from one to estimate precision on the other.
However, it might also be the case that $X_i$ and $X_j$ only overlap on a small region, in which case the samples from $X_j$ do not accurately represent instances in $X_i$ and could lead to a biased estimate.
We address this problem by using importance sampling \citep{owen2013monte}, a standard statistical technique for estimating properties of one distribution using samples from another distribution by reweighting those samples.

In particular, we propose the following the joint precision estimator:
\begin{align}
  \pih_i^{(j)} &= \sum_{j=1}^m \frac{w_{ij}}{n_{j}} \sum_{x \in \Xh_j} \frac{p_i(x) f(x)}{q_i(x)},
\end{align}
where $q_i(x) = \sum_{j=1}^m w_{ij} p_j(x)$ and $w_{ij} \ge 0$ are mixture parameters such that $\sum_{j=1}^m w_{ij} = 1$ and $q_i(x) > 0$ wherever $p_i(x) > 0$.
This last condition is easy to guarantee by setting $w_{ii} > 0$.
Intuitively, $q_i(x)$ behaves like the marginal distribution over $\sX$ if we drew samples the mixture of $p_j(x)$, with mixture weights $w_j$.

In \refapp{sampling} of the supplementary material, we prove that $\pih_i$ is an unbiased estimator of $\pi_i$ and also work out an expression for its variance. 
The variance depends on $f(x)$, but the general intuition is that $\pih_i$ will have high variance if $q_i(x) \ll p_i(x)$.
In particular, we can show that the ideal choice of $w_{ij}$ for if $X_i$ and $X_j$ are disjoint is $0$, and if $X_i$ and $X_j$ are identical is $w_{ij} \propto n_{j}$.
This motivates the choice $w_{ij} \propto n_{j} \sum_{x \in \sX} p_j(x) p_i(x)$.

\subsection{Estimating recall}
Let $\Yh_1, \ldots, \Yh_m$ be the intersection of $\Xh_i$ with $\sY$.
While $\Yh_i$ form a randomly sampled subset of $\sY$ that we could evaluate recall on, it is a very biased subset.
To see this, consider the case where the pool $\Yh$ is formed from a single system; here that system would be given a perfect recall score, irrespective of how good it is.
The magnitude of this bias may decrease with more and more systems, but it is unwise to assume that the pool of systems sufficiently covers all the elements of $\sY$.\footnote{
  In particular, for the task of KBP, the recall of systems is bottlenecked by the NER systems they use, etc.
}
We correct for the bias by adjusting the recall of a system measured against the pool $Y$ (the \textit{pooled recall}), $\nu_i = \E_{x\sim Y}[g_i(x)]$, by multiplying it by the recall of the pool itself, $\theta = \E_{x\sim\sY}[\I[x \in Y]]$: $\rho_i = \theta \nu_i$.
Our proposed estimator estimator for recall $\rho_i$ is then $\rhoh_i \eqdef \thetah\nuh_i$, where $\thetah$ is $\sum_{x \in \Yh_0} \I[x \in \Union_{i=1}^m X_i]$ and $\nuh_i$ is a self-normalized importance-weighted estimator:
\begin{align*}
  \nuh_i &\eqdef \frac{\sum_{j=1}^m \frac{w_j}{n_j} \sum_{x \in \Yh_j} \frac{u(x) g_i(x)}{q(x)}}{\sum_{j=1}^m \frac{w_j}{n_j} \sum_{x \in \Yh_j} \frac{u(x)}{q(x)}},
\end{align*}
where $p'(x) \propto u(x)$, $q(x) = \sum_{j=1}^m w_{j} p_j(x)$ and $w_{j} \ge 0$ are mixture parameters such that $\sum_{j=1}^m w_{j} = 1$. We use $w_j \propto n_j$.
In \appendixref{sampling}, we show that $\rhoh_i$ is unbiased.

\subsection{Adaptively drawing samples for new sets}
Finally, the desired property for our framework is to annotate new data only when necessary, i.e.\ a new submission $X_m$ contains sufficiently diverse output.
We formalize this statement by requesting for a target variance $\epsilon$ for our precision estimates.
The variance of $\pih_m$ is a complex non-convex function, but we know that it is monotonically decreasing in $n_m$, the number of samples drawn from the new output, $X_m$.
Consequently it is quite easy to solve for the number of samples needed to achieve a target variance of $\epsilon$ using a bisection method.
