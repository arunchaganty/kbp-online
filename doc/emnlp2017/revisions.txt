* Prove heuristic choices are optimal at extremes. 

--------

* [?] Question about platform dynamics: who pays for what?
  - It is a feature of our approach that annotation costs decrease over
    time. We are currently piloting the service at no cost to
    researchers.
* [?] On biasing evaluation based on previous system outputs -- I think there might
be some work by Barbara Plank on this for syntactic annotations?

* [?] How could other practitioners implement IS method themselves.
  connect the actual technical method in Sec 4 to the motivating statements about it from the introduction.

--------


* [/] It was hard to tell what the CIs meant.  Are these CIs that a practitioner would calculate (and therefore their coverage and biases should be evaluated)?  Or were these a measure of variability in what the evaluation method's point estimates would be, when simulating a data generation and evaluation process?  Figure 5 was a little hard to read with the CIs.  "approximately" in L684-686: why not just calculate widths of the actual CIs (difference in percentiles)?  The authors' arguments are about bias and variance; might as well make them quantitatively instead of relying on a hard to read figure for a purely visual presentation.
  - The CIs plotted in Figure 5 are over 500 different simulations of the experiment described in section 6.1. The intervals change from 0.135 to 0.063 (precision) and from 0.141 to 0.081 (recall).
* [/] 5.2: what does interannotator agreement rate mean here, and why does it differ
with different numbers of annotators?  Same question for multiple worker
results in 5.3.
  - The inter-annotator rates are Fleiss’s kappa scores averaged over
    relation types. We apologize for an error in the reported figures;
    they should be 0.613 and 0.615 (both considered substantial
    agreement) for 3 and 5 raters respectively. 
* How was LDC data collected?
  - [/] The method used by LDC to collect the evaluation data is described
    in Xuansong et al. (LREC 2012) -- we will be sure to reference it.
    We will provide detailed comparisons with the existing evaluation in
    the appendix.
* [/] Math note: I think each \hat{X}_j (for system j) has to be a multiset, not a
set.  Line 356 says it contains "n_j i.i.d. samples".  Therefore it must not be
sampling without replacement, since that's not i.i.d.; but if it's sampling
with replacement, then the same item can be selected multiple times.
* [/] Clarifying our refinements to IS [R3]
  - Lines 408-430: Our refinements modify the IS procedure and not the
    proposal distribution. The standard IS approach requires us to only
    use samples from the distribution q_i(x); unfortunately, q_i(x)
    differs for each system i and changes whenever a new system is
    added, requiring new samples to be drawn each time. Our approach
    allows us to reuse samples drawn from individual systems using
    numerical integration.

* [/] Lines 425-428 (first sentence of paragraph): not clear why this is true.
  - Our observations on w_ij follow from the variance expression in
    Theorem 1 (appendix B). Intuitively, if system A and B’s predictions
    are disjoint, B’s samples can have no effect in computing A’s
    _precision_. On the other hand, if A and B are identical but B has
    10x samples, using B’s samples more would reduce variance.
* [/] The p_i distributions: it is not clear what these mean on a SUBSTANTIVE basis.  We care about evaluation metrics that have already been defined, right?
  - Both the official TAC-KBP evaluation and our own use macro-averaged
    precision and recall as metrics. However, in the official
    evaluation, these metrics are computed over a fixed set of query
    entities chosen by LDC annotators, resulting in two problems: (a)
    defining query entities requires human intervention and (b)
    typically a large source of variability in evaluation scores comes
    from not having enough query entities (see e.g. Webber, W.E.
    (2010)). In our methodology, we replace manually chosen query
    entities by sampling entities from each system’s output. In effect,
    p_i makes explicit the decision process of the annotator who chooses
    query entities.
* [/] What data and schema are we using?
  - We would like to highlight that our framework also produces
    a growing dataset of annotations that are useful for relation
    extraction, entity detection, linking and coreference. The data
    conforms to the TAC-KBP schema with a few simplifications and will
    be available through our website.
* [/] Proofs in the appendix -- I did not check them in detail, though I noticed
several of the lemmas make claims like "X approximately equals Y".  That's not
a mathematically precise statement.  For example, Lemma 3 is really saying "X
has a first order Taylor approximation of Y", which is reasonable enough.

