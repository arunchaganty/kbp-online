\section{Related work}
\label{sec:related}

%Knowledge base population bears several similarities with information retrieval: we are also interested in answering a specific information need, a relation, from a large document corpus.
%It follows that research into information retrieval evaluation is extremely relevant.
% Axes of comparison:
% - identifying and measuring pooling bias
% Origin of pooling: K. Sp¨ arck Jones and C. J. van Rijsbergen. Report on the need for and provision of
%an ‘ideal’ test collection. Technical report, University Computer Laboratory, Cam- bridge, 1975
The subject of pooling bias has been extensively studied in the information retrieval (IR) community starting with \citet{zobel1998reliable}, which examined the effects of pooling bias on the TREC AdHoc task, but concluded that pooling bias was not a significant problem. %  and \citep{voorhees1999overview}
% zobel also mentions a reinforcing effect, but the dynamics are perhaps more dependent on the IR evaluation setup.
However, when the topic was later revisited,
%\citet{buckley2004incomplete} found that pooling bias could significantly change system ranking, and 
\citet{buckley2007bias} identified that the reason for the small bias was because the submissions to the task were too similar; upon repeating the experiment using a novel system as part of the TREC Robust track, they identified a 23\% point drop in average precision scores!%
\footnote{For the interested reader, \citet{webber2010measurement} presents an excellent survey of the literature on pooling bias.}
%We also use the methodology of \citet{zobel1998reliable} to measure pooling bias in KBP, but find that the pooling bias more significant of a problem in KBP evaluation than it has been in IR evaluation.
%%However, unlike in the information retrieval setting, we find a very strong effect \pl{what does strong effect mean?}.
%One explanation for this difference is that unlike evaluation metrics for KBP, popular IR metrics are rank-weighted, and unassessed documents, typically be lower in the ranking, contribute less to the evaluation score.

% - solutions to pooling bias 
Many solutions to the pooling bias problem have been proposed in the context of information retrieval, e.g.\ 
%  changing the queries to be more specific \citep{buckley2007bias}, 
  adaptively constructing the pool to collect relevant data more cost-effectively \citep{zobel1998reliable,cormack1998efficient,aslam2006statistical}, or
  modifying the scoring metrics to be less sensitive to unassessed data \citep{buckley2004incomplete,sakai2008information,aslam2006statistical}.
Many of these ideas exploit the ranking of documents in IR which does not apply to KBP.\@
%Furthermore, the pooling bias persists in TAC KBP evaluations even if \textit{all answers} reported by systems for a given set of entities are assessed for correctness.
While both \citet{aslam2006statistical} and \citet{yilmaz2008simple} estimate evaluation metrics by using importance sampling estimators, the techniques they propose require knowing the set of all submissions beforehand.
In contrast, our on-demand methodology can produce unbiased evaluation scores for new development systems as well.

%While pooling bias hasn't been studied on KBP before, \cite{surdeanau} does study the effect of incomplete labels on distant supervision.
% - crowdsourcing kbp and nlp
There have been several approaches taken to crowdsource data pertinent to knowledge base population \citep{vannella2014validating,angeli2014combining,he2015question,liu2016effective}.
The most extensive annotation effort is probably \citet{pavlick2016gun}, which crowdsources a knowledge base for gun-violence related events.
In contrast to previous work, our focus is on \textit{evaluating systems}, not collecting a dataset.
%As a result, we are able to integrate the systems we are evaluating while the data collection process.
Furthermore, our main contribution is not a large dataset, but an evaluation service that allows anyone to use crowdsourcing predictions made by their system. 

\section{Discussion}
\label{sec:discussion}

Over the last ten years of the TAC KBP task, the gap between human and system performance has barely narrowed despite the community's best efforts: top automated systems score less than 36\% \fone{} while human annotators score more than 60\%.
% Cite Weber -- IR has much still to progress, hidden by variability in scores, etc.
% Prevalence of patterns based systems.
In this paper, we've shown that the current evaluation methodology may be a contributing factor because of its bias against novel system improvements.
The new on-demand framework proposed in this work addresses this problem by obtaining human assessments of new system output through crowdsourcing.
The framework is made economically feasible by carefully sampling output to be assessed and correcting for sample bias through importance sampling.

% Presence of this platform allows us to decouple dataset creation from task. -- e.g. can use and evaluate distantly supervised datasets more easily.
Of course, simply providing better evaluation scores is only part of the solution and it is clear that better datasets are also necessary.
However, the very same difficulties in scale that make evaluating KBP difficult also make it hard to collect a high quality dataset for the task.
As a result, existing datasets \citep{angeli2014combining,adel2016comparing} have relied on the output of existing systems, making it likely that they exhibit the same biases against novel systems that we've discussed in this paper.
We believe that providing a fair and standardized evaluation platform as a service
allows researchers to exploit such datasets and while still being able to accurately measure their performance on the knowledge base population task.

% There are tasks that are much harder to evaluate in NLP -- generation tasks like summarization and dialogue. Much more important to do so as current metrics are even more unsatisfying.
% Poses a new challenge to be tackled -- being able to reuse assessments in KBP.
% would be valuable and hope that ideas of sampling and sample reuse will help.
%Finally, despite the challenges in evaluating knowledge base population we've described in this work,
There are many other tasks in NLP that are even harder to evaluate than KBP.\@
Existing evaluation metrics for tasks with a generation component---such as summarization or dialogue---leave much to be desired.
\ac{I kinda want to use Barbara Plank's \citep{plank16nonstandard} work as an example of how we need to evaluate our NLP systems out in the wild, and on-demand evaluation lets us do this. It certainly has for coreference and NER, but I don't know if we can extend that argument.}
We believe that adapting the ideas of this paper to those tasks is a fruitful direction,
as progress of a research community is strongly tied to the fidelity of evaluation.
%particular, we think that the community would be greatly aided by a more faithful evaluation methodology for
%We believe the sampling ideas presented in this work could be adapted,
%will still be relevant in such a setting,
%but the challenge to tackle is in being able to reuse assessments obtained on one summary for another.
