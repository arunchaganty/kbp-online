\section{Evaluation}
\label{sec:evaluation}

\subsection{Mock evaluation}
Let us now see how well our new evaluation framework works in practice.

We conduct a mock evaluation using 15,000 newswire documents from 2016 TAC-KBP competition.
We compare the predictions made by three distinct relation extraction systems: a rule-based system, a supervised system and a neural network classifier.
Each system uses Stanford CoreNLP~\citep{} to identify entities and the Illinois Wikifier~\citep{} to perform entity linking. 

\begin{table*}
  \centering
  \begin{tabular}{l l c c c} \toprule
    Scheme      & System    & $P^e (\pm 95\%)$ & $R^e (\pm 95\%)$ & $\fone{}^e (\pm 95\%)$ \\ \midrule
\multirow{3}{*}{Uncombined} &
  Patterns   & \fake{80.4 $\pm$ 3.0}\% & \fake{10.4 $\pm$ 5.0}\% & \fake{18.41 $\pm$ 4.3}\% \\
& Supervised & \fake{60.4 $\pm$ 3.0}\% & \fake{15.4 $\pm$ 5.0}\% & \fake{24.54 $\pm$ 4.3}\% \\
& Neural     & \fake{20.4 $\pm$ 3.0}\% & \fake{30.4 $\pm$ 5.0}\% & \fake{24.41 $\pm$ 4.3}\% \\ \midrule
\multirow{3}{*}{+ Pooling} &
  Patterns   & \fake{80.4 $\pm$ 3.0}\% & \fake{10.4 $\pm$ 3.0}\% & \fake{18.41 $\pm$ 3.0}\% \\
& Supervised & \fake{60.4 $\pm$ 3.0}\% & \fake{15.4 $\pm$ 3.0}\% & \fake{24.54 $\pm$ 3.0}\% \\
& Neural     & \fake{20.4 $\pm$ 2.6}\% & \fake{30.4 $\pm$ 2.7}\% & \fake{24.41 $\pm$ 2.6}\% \\ \bottomrule
  \end{tabular}
  \caption{\label{tbl:evaluation-results} Results from a mock evaluation.}
\end{table*}

In total, 100 documents were exhaustively annotated for about \$2,000, and 1000 of each systems submissions were annotated at about \$300 each.
\tableref{evaluation-results} presents the results of these systems on the mock evaluation.
%\footnote{A summary of mention-level scores \fake{can be found in the appendix}.}  
% How does cost compare?
% How do absolute scores compare?
Two immediate takeaways are that the precisions of these systems are \fake{on par with their precisions} in the original 2015 evaluation but the 95\% confidence interval is \fake{almost a third as large}.
The recall scores on this evaluation are \fake{a bit smaller than on the 2015 evaluation} and again the 95\% confidence window is \fake{significantly smaller}.
% Score adjustment.
Combining annotation pooling annotations \fake{noticeably reduces} the variance over the uncombined estimation,
while combining the unassessed output \fake{makes a smaller impact}.

% TODO: plot scores per queries as a measure of query variability? How
% do we demonstrate that our artificial query scheme isn't rubbish?
