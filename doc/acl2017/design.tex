\section{A new evaluation methodology}
\label{sec:methodology}

In \sectionref{analysis} we identified two main problems with the current evaluation methodology:
\begin{enumerate}
  \item A closed-window pooling evaluation is significantly biased
    against development systems, particularly those of new teams.
  \item Existing evaluation metrics have too much variance on the
    evaluation data to be a measure of performance that will generalize
    to new documents and queries.
\end{enumerate}
Our goal in this section is to propose a new methodology that addresses both of these problems head on 
  by indefinitely opening the pooling evaluation window by making the evaluation online
  and by combining evaluation data for each team in statistically optimal fashion.

\paragraph{Would a larger human-annotated dataset suffice?}
A natural question to ask before embarking on the significant effort of an online evaluation is whether a large-enough dataset could be reliably collected by spending a little more money. 

An important part of the KBP evaluation is to test on an unseen document collection.

A number of datasets for relation extraction, even on the TAC-KBP task, have been collected in the past, for example in the work of \citet{angeli2014combining}.
This dataset is very small and does not have sufficient coverage, but is useful for training.
Unfortunately, these datasets have also been collected by pooling responses.

Another way of empirically answering this question is to see how many contexts need be evaluated to reduce pooling bias sufficiently. We can extrapolate this number by studying how the pooling bias varies by the number of systems.
Using the 2015 evaluation data, we find that we would need to pool the output of \todo{100} systems with \todo{$10^6$} lines of output evaluated.
With the `anydoc' heuristic, this number is far less, but the error introduced by the heuristic also increases.
%TODO: graph.

We can estimate the cost of annotating such a large corpus by calculating how many documents need be exhaustively annotated by humans-- it comes out to \todo{\$10 million}. 
We believe this is reasonable justification to discard the static dataset approach and embrace an online evaluation wherein systems are continuously evaluated.

\paragraph{Constructing an online evaluation.}

When constructing our online evaluation methodology, we must be very careful not to bias the evaluation data we produce towards any particular system. 
There are several modes in which we can obtain annotations on our data.

\begin{enumerate}
  \item \textbf{Exhaustive annotation.} 
    We can sample documents from the corpus and have crowdworkers exhaustively annotate them.
    There are three types of annotations that are required, corresponding to each component of the relation extraction task:
    detecting entities in documents, linking them within and across documents and identifying relations between entities in a document. 
    % Include details about the turk interfaces?
  \item \textbf{Pooling annotation.} 
    Exhaustively annotating documents is quite laborious and hence expensive to do.
    Verifying a systems' submission is much easier to do; it takes only \todo{10 seconds} a relation.
    It also reduces wasted effort in reading text that doesn't contain any relations.

    By including every new submission into the pool, we can avoid the negative consequences of pooling evaluation.
    Furthermore, we will show how annotations collected in the pooling and exhaustive schemes can be combined to estimate true recall and precision more cheaply.

  \item \textbf{Estimative annotation.} 
    Finally, we should be able to use agreements between submitting systems as a weak signal of correctness and be able to compute precision and recall on the entire corpus of unannotated contexts.
    One of the main challenges  we'll have to solve is to make sure that we aren't biasing our scores due to correlations between systems. 

    % TODO: explain using a pooling classifier?
    % Builds on the previous two systems.
\end{enumerate}

\paragraph{Combining annotations.}

Each of the annotation schemes above is advantageous for particular metrics: the exhaustive scheme is necessary for estimating true recall, the pooling scheme is important to reliably estimate precision and the estimative annotation is cheap.

However, we can combine the strengths of these three estimators to allow us to reliably evaluate systems at a fraction of the original cost.

Let us consider how data in the pooling scheme can be used to better estimate recall when combined with the exhaustive annotation.
Let us assume that we have annotated $n_e$ true contexts through the exhaustive annotation, and are able to evaluate the recall of a system $A$ is $r_A$, with some variance $\sigma^2_A$.
We are also able to evaluate the recall of the entire pool $P$ as $r_P$, with variance $\sigma^2_P$.
Let's assume that we can evaluate $n_p$ contexts at random from the pool. 
%The recall of $A$ within the pool is $r'_A$, with variance $\sigma'^2_{A}$.
%We could use the recall of $A$ in the pool to estimate the recall of $A$ as $r_A = r_P * r'_A$.
%The variance of this estimate is $\sigma^2_P \sigma'^2_A + r^2_P \sigma'^2_A + \sigma^2_P r'_A^2$.
%Let's take the limit where estimating pooled recall is very cheap -- in this case, we can take $\sigma'^2_A \to 0$, giving us a new estimate of $r'_A^2 \sigma^2_P$. 
%Noting that recall is a binomial estimator, 
%  we know that $\sigma^2_A = \frac{r_A (1 - r_A)}{n_e}$ and 
%  that $\sigma^2_P = \frac{r_P (1 - r_P)}{n_e}$.
%Thus, our two-step estimator has less variance than the one-step estimator when,
%  $\frac{r'_A^2 r_P (1 - r_P)}{n_e} < \frac{r'_A r_P (1 - r'_A r_P)}{n_e}$, or when $r_'A (1 - r_P) < (1 - r'_A r_P)$.

With a Bernoulli assumption, we get that.

Then, our updated estimate of recall would combine $r_A$ and 

% (2 pages)

\begin{table*}
  \begin{tabular}{l r r r r r r r r} \toprule
    Annotation scheme & Cost per sample & \multicolumn{6}{c}{Information ratio} & Total cost \\ 
                      &                      & $P^m$ & $R^m$ & $F_1^m$ & $P^e$ & $R^e$ & $F_1^e$ &  \\ \midrule
    Exhaustive & \$0.20 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & \$1000 \\
    Selective  & \$0.05 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & \$1000 \\
    Estimative & \$0.00 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 &  \\
    Hybrid     & \$0.06 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & \$1000 \\ \bottomrule
  \end{tabular}
  \caption{Cost accuracy tradeoffs}
\end{table*}

A motivation for pooling is to reduce cost.
The intuition is in computing recall, we can compute recall against a particular corpus of exhaustively annotated documents,
or compute the recall of the pool with respect to the exhaustive annotations, followed by evaluating the recall of with respect to the pool. 
The latter is much cheaper to evaluate, and as a result, we can get a much more precise annotation on it.
Consider this example to gain intuition as to why incorporating pooling results would help.

Intuition is that if we take two sets of documents and ask for overlap, magic 

