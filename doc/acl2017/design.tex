\section{A new evaluation methodology}
\label{sec:methodology}

In \sectionref{analysis} we identified two main problems with the current evaluation methodology:
\begin{enumerate}
  \item A closed-window pooling evaluation is significantly biased
    against development systems, particularly those of new teams.
  \item Existing evaluation metrics have too much variance on the
    evaluation data to be a measure of performance that will generalize
    to new documents and queries.
\end{enumerate}

At the same time, we would like to capture some of the defining aspects of the original TAC-KBP evaluation:
\begin{enumerate}
  \item The evaluation should allow comparisons against human effort on the task. %TODO: reason?
  \item Scores should be aggregated over entities, because entity-level scores are a more natural measure of knowledge base quality.
  \item Teams should be required to submit justifications for relations they predict, because this makes it possible to interpret and verify system output.
\end{enumerate}

Our goal in this section is to propose a new methodology that maintains the above elements while addressing the mentioned problems head on 
  by indefinitely opening the pooling evaluation window by making the evaluation online
  and by combining evaluation data for each team in statistically optimal fashion.
Before we describe the overview of our proposed evaluation platform, we take a brief detour to answer an important question.

\paragraph{Would a larger human-annotated dataset suffice?}
A natural question to ask before embarking on the significant effort of an online evaluation is whether a large-enough dataset could be reliably collected by spending a little more money. 

An important part of the KBP evaluation is to test on an unseen document collection.

A number of datasets for relation extraction, even on the TAC-KBP task, have been collected in the past, for example in the work of \citet{angeli2014combining}.
This dataset is very small and does not have sufficient coverage, but is useful for training.
Unfortunately, these datasets have also been collected by pooling responses.

Another way of empirically answering this question is to see how many contexts need be evaluated to reduce pooling bias sufficiently. We can extrapolate this number by studying how the pooling bias varies by the number of systems.
Using the 2015 evaluation data, we find that we would need to pool the output of \fake{100} systems with \fake{$10^6$} lines of output evaluated.
With the `anydoc' heuristic, this number is far less, but the error introduced by the heuristic also increases.
%TODO: graph.

We can estimate the cost of annotating such a large corpus by calculating how many documents need be exhaustively annotated by humans-- it comes out to \fake{\$100,000}. 
We believe this is reasonable justification to discard the static dataset approach and embrace an online evaluation wherein systems are continuously evaluated.

\paragraph{Overview of the evaluation platform.}
The interface to our proposed evaluation platform is very similar to that of the TAC-KBP competition:
% input?
  participating teams are provided a document corpus with about $10,000$ documents
% output?
  and asked to submit a knowledge base consisting of every entity mention in the document along with links to canonical entity ids and relations between these entity mentions.
However, unlike the TAC-KBP competition, the submitted knowledge base is immediately evaluated for correctness and coverage by crowdworkers.
% metrics?
Teams obtain mention-level and entity-level scores within hours of their submission, allowing them to improve their systems immediately.
After several months of submissions, we plan to release the data annotated during this evaluation period and move on to a new evaluation corpus. 

There are two other significant departures from the current KBP specification.
First, we ask teams to submit every relation context justifying a predicted relation between two entities instead of just one:
  this additional output will be valuable in measuring the relation extraction quality independent of entity linking performance.
% queries?
Secondly, we evaluate entity-level scores on randomly sampled entities from documents instead of manually constructed queries.
The queries developed by the LDC were explicitly designed to be challenging, but unfortunately this is a subjective criterion that is hard to automate-- \fake{we show later that queries that are randomly sampled are still quite challenging to state of the art systems.}
Recent editions of the competition have included `two-hop' scores that evaluate how well the constructed knowledge base can answer queries that traverse two relational edges, e.g.\ ``What was Carrie Fisher's mother professional title?''
Evaluation using these two hop queries is currently out of scope for this work.

Next, we will describe how crowdworkers evaluate the submitted output, starting first with precision and followed by recall.

\begin{figure*}
\begin{subfigure}{0.31\textwidth}
  \includegraphics[width=\textwidth]{figures/extraction-interface}
  \caption{\label{fig:entity-interface} Entity detection and linking.}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\textwidth}
  \includegraphics[width=\textwidth]{figures/relation-interface}
  \caption{\label{fig:relation-interface} Relation extraction.}
\end{subfigure}
\hfill
\begin{subfigure}{0.31\textwidth}
  \includegraphics[width=\textwidth]{figures/relation-interface}
  \caption{\label{fig:linking-interface} Cross-document linking.}
\end{subfigure}
\caption{\label{fig:interfaces} Screenshots of the annotation interfaces.}
\end{figure*}

\paragraph{Evaluating precision with pooled annotation.}
% Actual interface to verify turker output.
Entries in the submitted knowledge base are verified by crowdworkers using the context provided.   
Using an annotation interface shown in \figureref{relation-interface},
crowdworkers are asked to verify 
  whether the participating mentions in the relation are valid entities,
  whether the predicted relation holds between the two mentions and
  whether the mentions are correctly linked if the mention is linked to a Wikipedia entry.
If the participating mention is not linked to a Wikipedia entry,
then we ask crowdworkers to resolve cluster assignments using a separate interface (\figureref{linking-interface}).
On average, we find that crowdworkers are able to perform this task at about \fake{10 seconds an entry}, corresponding to about \fake{\$0.10 per entry}, and we need about \fake{3 workers per entry} to achieve an inter annotator agreement of \fake{0.90}.

% We need to sample output.
Each system may output many thousands of relations-- evaluating every one of these relations would be prohibitively expensive
  and ultimately unnecessary if we are willing to only provide metric scores up to a certain degree of confidence.
% How much output to sample?
We propose instead to sample predicted relations and their contexts from each system's output to achieve a desired confidence interval.
% how to extend to entities?
However, uniformly sampling over entries would cause popular entities to be overrepresented in our entity-level scores.
Instead, we first uniformly sample entities from the output, then sample fills from each entity and mentions for each fill (see \figureref{evaluation-table} for a visual representation of this clustering).
Precision scores of each fill (measured over its mentions) are aggregated to compute the precision score of that entity.
It is easy to reuse these stratified samples to evaluate mention-level metric scores using importance reweighing. 
Annotating 1,000 contexts is sufficient for a confidence interval of about 3\% points and comes at a much more reasonable cost of about \$100 for a submission. 
% how to reuse labels and amortize costs?
Furthermore, we can reuse annotations on entries from previous submissions\footnote{
These annotations must be importance reweighed to eliminate bias.},
  allowing the cost per submission to decrease with increasing numbers of submissions.

\paragraph{Evaluating recall with exhaustive annotation.}
The pooled collection of relations from submissions is unfortunately still a very poor proxy for the set of all possible relations in the document corpus.
Consequently, we propose having crowdworkers attempt to exhaustively annotate entities and relations on a random sample of about 100 documents.
Crowdworkers begin by identifying every mention span in a document and specifying its type using the interface shown in \figureref{entity-interface}. For each mention, they are also asked to identify the canonical mention (i.e.\ perform coreference) within the document and identify links to Wikipedia pages.
Every pair of mentions within a sentence\footnote{
  We found that an overwhelming majority of relations were expressed in a single sentence.
  The quadratic blowup in overhead of considering mentions across two sentences is simply not worth it.}
  with compatible types is then shown to a crowdworker, who is then requested identify the relation between the two mentions using the relation identification interface (\figureref{relation-interface}).
  Finally, we ask crowdworkers to cluster entities that could not be linked to Wikipedia using the clustering interface (\figureref{linking-interface}).
  In this interface, crowdworkers are shown canonical mentions for every unlinked entity cluster from the small document corpus.

These interface are far more involved and consequently take longer for crowdworkers to complete.
The entity annotation interface takes on average about \fake{10 minutes per document}, corresponding to about \fake{\$2.00 a document}, and 
again required \fake{3 annotators} to get to an inter-annotator agreement of \fake{0.90}.
The relation annotation interface takes on average about \fake{3 minutes per document}, corresponding to about \fake{\$0.60 a document}, and required \fake{3 annotators} to get to an inter-annotator agreement of \fake{0.90}.
Finally, the clustering interface takes on average about \fake{6 minutes for 100 documents}, corresponding to about \fake{\$1.20 a document}, and required \fake{3 annotators} to get to an inter-annotator agreement of \fake{0.90}.

There are two simplifications we adopt in our interfaces.
Firstly, we exclude the extremely rare relations corresponding to ``cause of death'', ``has criminal charge'', ``age'' and ``number of employees'' and correspondingly their slot types.
In total, annotators specify one of five types, ``Person'', ``Organization'', ``City, State or Country'', ``Date'' and ``Title'', with the latter being a very common example of an open-category non-named type.
Secondly, we do not allow crowdworkers to annotate nested spans, preventing them from identifying both the organization and the city in \textit{``[University of [Chicago]]''}. 

Unfortunately, a uniformly random sample of documents from the corpus is unlikely to contain any cross-document links for any but the most popular of entities.
Consequently, while constructing the document collection, we first randomly sample about 20 documents and annotate them.
From the set of named entities identified, we uniformly sample (without replacement) an entity and then uniformly sample up to 4 additional documents that mention this entity by name and annotate them.
We repeat this procedure until we have 100 documents.
While this sampling procedure is not representative of the whole corpus, it does increase the probability of having cross-document links for less popular entities, capturing that criteria from the TAC-KBP evaluation queries.

\paragraph{Reducing variance by incorporating pooled and unassessed output.}

%    Finally, we should be able to use agreements between submitting systems as a weak signal of correctness and be able to compute precision and recall on the entire corpus of unannotated contexts.
%    One of the main challenges  we'll have to solve is to make sure that we aren't biasing our scores due to correlations between systems. 
%
%    % TODO: explain using a pooling classifier?
%    % Builds on the previous two systems.
%\end{enumerate}

Each of the annotation schemes above is advantageous for particular metrics: the exhaustive scheme is necessary for estimating true recall, the pooling scheme is important to reliably estimate precision and the estimative annotation is cheap.

However, we can combine the strengths of these three estimators to allow us to reliably evaluate systems at a fraction of the original cost.

Let us consider how data in the pooling scheme can be used to better estimate recall when combined with the exhaustive annotation.
Let us assume that we have annotated $n_e$ true contexts through the exhaustive annotation, and are able to evaluate the recall of a system $A$ is $r_A$, with some variance $\sigma^2_A$.
We are also able to evaluate the recall of the entire pool $P$ as $r_P$, with variance $\sigma^2_P$.
Let's assume that we can evaluate $n_p$ contexts at random from the pool. 
%The recall of $A$ within the pool is $r'_A$, with variance $\sigma'^2_{A}$.
%We could use the recall of $A$ in the pool to estimate the recall of $A$ as $r_A = r_P * r'_A$.
%The variance of this estimate is $\sigma^2_P \sigma'^2_A + r^2_P \sigma'^2_A + \sigma^2_P r'_A^2$.
%Let's take the limit where estimating pooled recall is very cheap -- in this case, we can take $\sigma'^2_A \to 0$, giving us a new estimate of $r'_A^2 \sigma^2_P$. 
%Noting that recall is a binomial estimator, 
%  we know that $\sigma^2_A = \frac{r_A (1 - r_A)}{n_e}$ and 
%  that $\sigma^2_P = \frac{r_P (1 - r_P)}{n_e}$.
%Thus, our two-step estimator has less variance than the one-step estimator when,
%  $\frac{r'_A^2 r_P (1 - r_P)}{n_e} < \frac{r'_A r_P (1 - r'_A r_P)}{n_e}$, or when $r_'A (1 - r_P) < (1 - r'_A r_P)$.

With a Bernoulli assumption, we get that.

Then, our updated estimate of recall would combine $r_A$ and 

% (2 pages)

\begin{table*}
  \begin{tabular}{l r r r r r r r r} \toprule
    Annotation scheme & Cost per sample & \multicolumn{6}{c}{Information ratio} & Total cost \\ 
                      &                      & $P^m$ & $R^m$ & $F_1^m$ & $P^e$ & $R^e$ & $F_1^e$ &  \\ \midrule
    Exhaustive & \$0.20 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & \$1000 \\
    Selective  & \$0.05 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & \$1000 \\
    Estimative & \$0.00 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 &  \\
    Hybrid     & \$0.06 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & \$1000 \\ \bottomrule
  \end{tabular}
  \caption{Cost accuracy tradeoffs}
\end{table*}

A motivation for pooling is to reduce cost.
The intuition is in computing recall, we can compute recall against a particular corpus of exhaustively annotated documents,
or compute the recall of the pool with respect to the exhaustive annotations, followed by evaluating the recall of with respect to the pool. 
The latter is much cheaper to evaluate, and as a result, we can get a much more precise annotation on it.
Consider this example to gain intuition as to why incorporating pooling results would help.

Intuition is that if we take two sets of documents and ask for overlap, magic 

