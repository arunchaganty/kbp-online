\section{On-demand open-world evaluation}
\label{sec:methodology}

We have shown that the bias in a closed-world evaluation poses an obstacle to innovating new systems.
We could eliminate the bias if we had exhaustively annotated the entire document corpus so that any relations predicted by systems would already be labeled.
Unfortunately, this would be a laborious and expensive task:
  using the interfaces we developed (described in detail in \refsec{evaluation}), it costs about \$30 to annotate a single document with crowdworkers, leading to an estimated cost of at least \$300,000 for a reasonably large corpus of 10,000 documents.
The annotation effort would cost significantly more with expert LDC-style annotation.

In on-demand open-world evaluation, the annotation process is automated so that new predictions can be labeled \textit{when necessary}.
When a new system or feature has been developed, its predictions are submitted to an evaluation portal.
If the submitted predictions can not be fairly and accurately evaluated with the data obtained so far,
  annotations are immediately obtained for a sample of the predicted relations through crowd-sourcing and added into the evaluation set.
In this way, we push the hard work of finding candidate relations to automated systems and instead focus annotation effort on the much easier task of verifying relation predictions.
At the end of this process, systems are guaranteed to obtain fair instance-level, relation-level and entity-level scores.

\subsection{Problem statement}

Let us define the task formally.
Let $\sX$ be a universe of possible outputs (e.g.\ relation instances),
  $X \subseteq \sX$ be a known subset that corresponds to the predicted output from a system,
  and $\sY \subseteq \sX$ be an unknown subset of this universe corresponding to the correct elements in $\sX$.
In on-demand open-world evaluation, we are allowed to ask if $x \in \sY$ or for samples from $\sY$ for a certain cost and must estimate the precision, $\pi$, and recall, $\rho$, of the predictions $X$ within a certain confidence interval $\epsilon$.

Formally, let $f(x) \eqdef \I[x \in \sY]$ and $g(x) = \I[x \in X]$, then:
\begin{align*}
  \pi  &\eqdef \E_{x \sim X}[f(x)] &
  \rho &\eqdef \E_{x \sim \sY}[g(x)],
\end{align*}
where $x$ is sampled from $X$ and $\sY$ according to distributions $p(x)$ and $p'(x)$ respectively.
We assume that $p(x)$ is known, e.g.\ the uniform distribution over $X$, and that samples from $p'(x)$ can be obtained, even if it is unknown.

Finally, to make evaluation cost-effective, we would like to reduce the combined cost of evaluating $m$ sets of predictions from different systems, $X_1, \ldots X_m \subseteq \sX$, with corresponding distributions $p_1, \ldots p_m$.

\paragraph{Estimation via sampling.}

By expressing precision and recall as expectations above, we can evaluate these metrics by sampling from $X$ and $\sY$ and evaluating $f(x)$ and $g(x)$ respectively.
Indeed, it is hard to do better for a single $X$ without additional information regarding $X$ or the system that produced it.
However, standard statistics tells us that to estimate $\pi$ and $\rho$ to within $\epsilon$ requires about $1/\epsilon^2$ samples.
If we want $\epsilon$ to be less than $0.01$ (1\%), we need at least 10,000 samples which can be quite costly to obtain on a per-system basis.
On the other hand, if all the sets and distributions were identical, than the individual costs could be amortized by a factor of $1/m$ by using the same samples across all the $X_i$.
In the next two parts we discuss how we can reuse samples when all the $X_i$ are different.

\subsection{Importance reweighting in estimating precision}
\newcommand{\Xh}{{\hat{X}}}
\newcommand{\fh}{{\hat{f}}}
\newcommand{\muh}{{\hat{\mu}}}

Let us suppose we have collected samples $\Xh_i$ from the sets $X_i$ according to the distribution $p_i(x)$ for each $i$.
Clearly each $\Xh_i$ can be used to estimate $\pi_i$, but can we use the samples obtained from $X_i$ when estimating $\pi_j$?
To do so, we'll exploit importance reweighing that allows us to estimate $\E_{p_1}[f(x)]$ using samples from $p_2(x)$ by using the estimator: $\frac{1}{n}\sum_{i=1}^n \frac{p_1(x_i)}{p_2(x_i)} f(x_i)$ 


% TODO: Figure that compares our scheme with equivalent of pooling.
We would like to combine the different 
Let us start by estimating precision.

To address this, we will consider the distribution that is a \textit{mixture} of existing distributions and some samples from the target set to guarantee support coverage.
Let $\pi = (\pi_1, \ldots, \pi_m)$ be a valid mixture distribution (i.e. $\pi_i \ge 0, \sum_{i} \pi_i = 1$),
and let the proposal distribution be $q(x) = \sum_i \pi_i p_i(x)$.
The sample set we are estimating from is $Y = (\Xh_1, \ldots, \Xh_m)$, which consists of samples drawn from every $X_i$.
We would like to find the optimal $\pi$ for a $\mu_i$.
Note that we have sampled our sets $X_i$ before hand and would like to fuse them together optimally.
This is different than if we were considering sampling from $q(x)$: the optimal distribution to estimate $\mu_i$ would of course be $p_i$.

Our proposed estimator is $\muh_i = \sum_{j=1}^m \frac{\pi_j}{|\Xh_j|} \sum_{x \in \Xh_j} \frac{p_i(x)}{q(x)} f(x)$.
First, let's confirm that this estimator is unbiased:
\begin{align*}
  \E_{Y}[\muh_i] 
  &= \sum_{j=1}^m \frac{\pi_j}{|\Xh_j|} |\Xh_j| \E_{p_j}[\frac{p_i(x)}{q(x)} f(x)] \\
  &= \sum_{j=1}^m \pi_j \sum_{x \in \sX} p_j(x) \frac{p_i(x)}{q(x)} f(x) \\
  &= \sum_{x \in \sX} (\sum_{j=1}^m \pi_j p_j(x)) \frac{p_i(x)}{q(x)} f(x) \\
  &= \sum_{x \in \sX} q(x) \frac{p_i(x)}{q(x)} f(x) \\
  &= \sum_{x \in \sX} p_i(x) f(x) \\
  &= \mu.
\end{align*}

Now let's compute the variance.
Let  $\mu_{ij} = \E_{p_j}[\frac{p_i(x)}{q(x)} f(x)]$, so that $\mu_i = \sum_i \pi_i \mu_{ij}$.
First of all, let's consider the variance in estimating a single pair: 
\begin{align*}
  \Var_{p_j}[\frac{p_i(x)}{q(x)} f(x)]
  &= \sum_{x \in \sX} p_j(x) \frac{{(p_i(x) f(x))}^2}{{q(x)}^2} - \mu_{ij}^2 \\
  &= \sigma_{ij}^2.
\end{align*}

Now, we also know that $\frac{1}{|\Xh_j|} \sum_{x \in \Xh_j} \frac{p_i(x)}{q(x)} f(x)$ will have variance $\frac{\sigma_{ij}^2}{|\Xh_j|}$; to keep expressions simple, we'll roll in this scaling factor into $\sigma_{ij}^2$.
Note that both $\mu_{ij}$ and $\sigma_{ij}^2$ depend on $\pi$ through $q(x)$.

The variance of $\muh_i$ is now:
\begin{align*}
  \Var_{Y}[\muh_i]
  &= \sum_{j=1}^m \pi_j^2 \sigma_{ij}^2,
\end{align*}
because each set $\Xh_i$ is independent.

Some takeaways are that $\pi_{j} \propto \sigma_{ij}^{-2}$ and that $\sigma_{ij}$ depends on the difference in how much mass $p_j$ places on $f(x)$.



\subsection{Unbiased estimation of recall}
% TODO: Figure that compares our scheme with equivalent of pooling.

\subsection{Application to evaluating KBP}
% TODO: include specific details to evaluating on KBP.

%There are two key problems to address in on-demand open-world evaluation.
%%There are two further problems we must address. %three inter-related challenges that we must tackle.
%First, we must ensure that our estimates of recall accurately reflect open world recall despite us not knowing the complete set of true relations.
%% being evaluating on an evaluation set constructed from system predictions.
%% We achieve this by calibrating our scores on a small set of exhaustively annotated documents.
%Secondly, we must ensure that the relations we sample allow us to compute unbiased precision scores.
%%entity-level and relation-level precision scores.
%%  We achieve this through structured sampling.
%%$Finally, we would to implement our evaluation framework cost-effectively.
%%  We achieve this by combining annotations across systems and sampling schemes through importance reweighing.
%We discuss our solutions to these problems next.

%On the other hand, it is much easier to verify a relation instance predicted by a system.
% TODO: outline the structure of the framework better.
% TODO: this should mention pooled annotations, etc.


\subsection{Ensuring open-world recall with exhaustive annotations}
% A line about pooled recall
\pl{I think we should use more formal notation from before to be more precise}

The pooled collection of evaluated relations in the evaluation set is unlikely to be an accurate estimate for all the true relations in the corpus and thus the pooled recall would be a biased estimate.
At the same time, it is impossible to know how many true relations there are in the corpus without annotating it in its entirety.
We address this problem by obtaining exhaustive annotations on a few documents to correct for the bias in using pooled recall.
% vv is possibly too much detail.
%For each sampled document, a crowdworker annotates any entities he/she can find.
%In a separate task, crowdworkers are then asked to identify relations between every pair of mentions in a single sentence.
%%\footnote{%
%%While there certainly are relations across sentences, this requires crowdworkers to evaluate many more mention pairs for relations.}
%During entity annotation, crowdworkers also link entities to Wikipedia pages if possible, allowing us to perform entity-level evaluation.

%We first evaluate whether exhaustive annotations can be reliably performed by crowdworkers.
%To do so, we compare crowdsourced annotations against those of expert annotators using data from the TAC-KBP 2015 EDL task~\citep{}.\footnote{%
%Further details regarding the annotation interface and experiment can be found in \refsec{turk}.
%}
%We find that crowdworkers identify 90\% of the entity spans identified by expert annotators and have significant token-level inter-annotator agreement (\fake{$\kappa = 0.8$}), validating the hypothesis that exhaustive annotation crowdsourced.

Then, when evaluating recall, we first estimate recall of the entire evaluation set on the exhaustively annotated documents, and multiply that by the recall of the system on the evaluation set. Through this two-step process we exploit the fact that it is easier to reduce variance by increasing the size of the evaluation set than it is to increase the size of the exhaustive annotation. \refsec{power} provides more details about this variance reduction.

\pl{Sounds too much like an ad-hoc side note;
this should be part of the problem definition (desired evaluation metric that we're trying to estimate),
not part of our solution; in other words, should be moved to a previous section
}
A final issue to discuss is how documents should be sampled to capture diverse entities that span documents. % to provide better entity-level scores.
When considering uniformly sampled documents, we that find that only extremely frequent entities like the United States or Barack Obama appear across documents.
Unfortunately, we do not know which entities are present in which documents to construct a fairer sampling scheme.
As a heuristic, the 20\% of our exhaustive document collection is sampled uniformly and annotated.
We then uniformly sample the entities annotated to create a collection of ``query entities''.
Finally, we construct the remaining 80\% of our document collection by searching for documents that contain the query entities according to an exact string match.
% TODO: would be nice to quantify 

\subsection{Structured sampling for entity-level and relation-level precision}
Estimation of relation-level and entity-level scores respectively assumes that relation types and entities are sampled uniformly.
However, if we were to uniformly sample relations from a typical system's predictions, 
  we would get a very skewed distribution over relation types and entities and thus bias our estimates.
We address this problem through two structured sampling schemes.

First, for relation-level scores, we stratify the predicted instances by relation and collect the same number of samples from each relation.
Secondly, for entity-level scores,  we first sample subject entities uniformly from the predicted KB, then sample fills for that entity, followed by instances for the fill.
For each such entity, we draw several samples.
% TODO: show a diagram.

Similarly, the distribution over relation instances for each system and scoring scheme (i.e.\ entity-level and relation-level) will be quite different. We correct for bias through importance reweighing~\citep{}.
