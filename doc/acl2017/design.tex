\section{A new evaluation methodology}
\label{sec:methodology}

In \sectionref{analysis} we identified two main problems with the current evaluation methodology:
\begin{enumerate}
  \item A closed-window pooling evaluation is significantly biased
    against development systems, particularly those of new teams.
  \item Existing evaluation metrics have too much variance on the
    evaluation data to be a measure of performance that will generalize
    to new documents and queries.
\end{enumerate}
Our goal in this section is to propose a new methodology that addresses both of these problems head on 
  by indefinitely opening the pooling evaluation window by making the evaluation online
  and by combining evaluation data for each team in statistically optimal fashion.

\paragraph{Would a larger human-annotated dataset suffice?}
A natural question to ask before embarking on the significant effort of an online evaluation is whether a large-enough dataset could be reliably collected by spending a little more money. 

An important part of the KBP evaluation is to test on an unseen document collection.

A number of datasets for relation extraction, even on the TAC-KBP task, have been collected in the past, for example in the work of \citet{angeli2014combining}.
This dataset is very small and does not have sufficient coverage, but is useful for training.
Unfortunately, these datasets have also been collected by pooling responses.

Another way of empirically answering this question is to see how many contexts need be evaluated to reduce pooling bias sufficiently. We can extrapolate this number by studying how the pooling bias varies by the number of systems.
Using the 2015 evaluation data, we find that we would need to pool the output of \todo{100} systems with \todo{$10^6$} lines of output evaluated.
With the `anydoc' heuristic, this number is far less, but the error introduced by the heuristic also increases.
%TODO: graph.

We can estimate the cost of annotating such a large corpus by calculating how many documents need be exhaustively annotated by humans-- it comes out to \todo{\$10 million}. 
We believe this is reasonable justification to discard the static dataset approach and embrace an online evaluation wherein systems are continuously evaluated.

\paragraph{Constructing an online evaluation.}

When constructing our online evaluation methodology, we must be very careful not to bias the evaluation data we produce towards any particular system. 
There are several modes in which we can obtain annotations on our data.

\begin{enumerate}
  \item \textbf{Exhaustive annotation.} 
    We can sample documents from the corpus and have crowdworkers exhaustively annotate them.
    There are three types of annotations that are required, corresponding to each component of the relation extraction task:
    detecting entities in documents, linking them within and across documents and identifying relations between entities in a document. 
    % Include details about the turk interfaces?
  \item \textbf{Pooling annotation.} 
    Exhaustively annotating documents is quite laborious and hence expensive to do.
    Verifying a systems' submission is much easier to do; it takes only \todo{10 seconds} a relation.
    It also reduces wasted effort in reading text that doesn't contain any relations.

    By including every new submission into the pool, we can avoid the negative consequences of pooling evaluation.
    Furthermore, we will show how annotations collected in the pooling and exhaustive schemes can be combined to estimate true recall and precision more cheaply.

  \item \textbf{Estimative annotation.} 
    Finally, we should be able to use agreements between submitting systems as a weak signal of correctness and be able to compute precision and recall on the entire corpus of unannotated contexts.
    One of the main challenges  we'll have to solve is to make sure that we aren't biasing our scores due to correlations between systems. 

    % TODO: explain using a pooling classifier?
    % Builds on the previous two systems.
\end{enumerate}

\paragraph{Combining annotations.}

Each of the annotation schemes above is advantageous for particular metrics: the exhaustive scheme is necessary for estimating true recall, the pooling scheme is important to reliably estimate precision and the estimative annotation is cheap.

However, we can combine the strengths of these three estimators to allow us to reliably evaluate systems at a fraction of the original cost.

Let us consider how data in the pooling scheme can be used to better estimate recall when combined with the exhaustive annotation.
Let us assume that we have annotated $n_e$ true contexts through the exhaustive annotation, and are able to evaluate the recall of a system $A$ is $r_A$, with some variance $\sigma^2_A$.
We are also able to evaluate the recall of the entire pool $P$ as $r_P$, with variance $\sigma^2_P$.
Let's assume that we can evaluate $n_p$ contexts at random from the pool. 
%The recall of $A$ within the pool is $r'_A$, with variance $\sigma'^2_{A}$.
%We could use the recall of $A$ in the pool to estimate the recall of $A$ as $r_A = r_P * r'_A$.
%The variance of this estimate is $\sigma^2_P \sigma'^2_A + r^2_P \sigma'^2_A + \sigma^2_P r'_A^2$.
%Let's take the limit where estimating pooled recall is very cheap -- in this case, we can take $\sigma'^2_A \to 0$, giving us a new estimate of $r'_A^2 \sigma^2_P$. 
%Noting that recall is a binomial estimator, 
%  we know that $\sigma^2_A = \frac{r_A (1 - r_A)}{n_e}$ and 
%  that $\sigma^2_P = \frac{r_P (1 - r_P)}{n_e}$.
%Thus, our two-step estimator has less variance than the one-step estimator when,
%  $\frac{r'_A^2 r_P (1 - r_P)}{n_e} < \frac{r'_A r_P (1 - r'_A r_P)}{n_e}$, or when $r_'A (1 - r_P) < (1 - r'_A r_P)$.

With a Bernoulli assumption, we get that.

Then, our updated estimate of recall would combine $r_A$ and 

% (2 pages)

\begin{table*}
  \begin{tabular}{l r r r r r r r r} \toprule
    Annotation scheme & Cost per sample & \multicolumn{6}{c}{Information ratio} & Total cost \\ 
                      &                      & $P^m$ & $R^m$ & $F_1^m$ & $P^e$ & $R^e$ & $F_1^e$ &  \\ \midrule
    Exhaustive & \$0.20 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & \$1000 \\
    Selective  & \$0.05 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & \$1000 \\
    Estimative & \$0.00 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 &  \\
    Hybrid     & \$0.06 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & 0.7 & \$1000 \\ \bottomrule
  \end{tabular}
  \caption{Cost accuracy tradeoffs}
\end{table*}

A motivation for pooling is to reduce cost.
The intuition is in computing recall, we can compute recall against a particular corpus of exhaustively annotated documents,
or compute the recall of the pool with respect to the exhaustive annotations, followed by evaluating the recall of with respect to the pool. 
The latter is much cheaper to evaluate, and as a result, we can get a much more precise annotation on it.
Consider this example to gain intuition as to why incorporating pooling results would help.

Intuition is that if we take two sets of documents and ask for overlap, magic 

\paragraph{Evaluation measures.} 
We would like to evaluate the knowledge base output reported by teams on three axes, \it{mention-level relation extraction}, \it{entity-level relation extraction} and \it{mention-level entity-linking}.

\paragraph{Mention-level relation extraction precision.}
For every relation mention returned by the system, we would like to evaluate the precision of the output, i.e. the likelihood that an output submitted by a system is correct: $P_m \eqdef \E_{x}[\correct(x)]$.
We derive an unbiased estimator for $P_m$ by randomly sampling output relation mentions, $\hat{P}_m = \frac{1}{n} \sum_{i}^n \correct(x_i)$, where $x_i$ are sampled uniformly from the submitted relations $X$.

\paragraph{Entity-level relation extraction precision.}
We now want to evaluate micro and macro-averaged relation extraction precision, aggregated over entities. 
To do so, we note that the entity level relation extraction requires three independent variables to be correct, the system must have correctly labeled the relation mention, and it must also correctly linked the subject and object mentions: $P_e \eqdef \E_{x}[ \correct_r(x) \correct_s(x) \correct_o(x)]$, which we evaluate as $P_e \eqdef \E_{x}[\correct_r(x)] \E_{x}[\correct_s(x)]\E_{x}[ \correct_o(x)] = P^r_m P^s_m P^o_m$.

\paragraph{Mention-level relation extraction recall.}
Recall is the probability that a correct mention-level relation is extracted,
  $R_m \eqdef \E_{x}[\outputted(x)]$, where $x$ are the set of true relation mentions.
To evaluate recall, we randomly sample $D'$ documents out of the corpus of $D$ documents.
Within each document, we have crowdworkers identify every relation in that document, also pooling any relations outputted by other systems. 
Thus our estimator is $\hat{R}_m = \frac{1}{n} \sum_{i}^n \outputted(x_i)$ 

\paragraph{Overcoming pooling bias without arbitrary scores.}

Doing a complete evaluation on a particular random set of documents.
Q. How much variation is there across documents? -- what is the right axis to vary over? 

\paragraph{Final protocol}

Set up is as follows:

* Every month, a new collection of $n$ documents is released online for users to evaluate their systems on.
* As the user uploads their system's $m$ predictions online, we will sample a fraction of these to predict the precision of their output.
* At the end of the month, the combination of every system's output is used to predict recall, allowing us to evaluate users based on F1.
* All evaluated output is released that month, augmenting the dataset of labeled instances.

Natural incentives:
* For systems to rank better overall, they must identify relations that other systems do not.

Economics:
  * Rate is about \$12/hr, which comes out to 0.3c/second.
  * To estimate precision within 1\% with 95\% probability, we need about $4*10^2 < 4 * 10^4 * p (1-p) < 10^4$ evaluated samples.
  * At face value, each example is answerable in 4--5 seconds, which is 1.5c. In total, that means \$150 per evaluation.
  * Amortized; let us say systems have an 80\% overlap in the slots produced (we need empirical backing here). Then, we can say each new evaluation costs about \$30. -- needs to be empirically verified.
  * However, we are budgeting \$20/evaluation. That means, trading off accuracy.
  * If instead, we go for a precision of about 2\%, we can achieve this under \$15.
