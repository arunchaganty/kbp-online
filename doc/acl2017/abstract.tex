\pl{don't like facts since that implies truth - changed to relation, since that's also used later in the paper;
be consistent}
\pl{added 'pooling bias' to abstract, since that's important!}

Large-scale information extraction systems for knowledge base population (KBP)
predict true relations from an unenumerable set of candidate relations from a large document corpus.
%These predictions have traditionally been evaluated under a closed-world assumption,
%wherein only a small annotated set of facts are considered to be true and all others false,
Due to the prohibitive cost of annotating the entire corpus,
KBP evaluation has been traditionally closed-world,
wherein a small subset of candidates are annotated, % to be positive or negative,
and candidates outside the subset are automatically marked negative.
In the annual TAC-KBP challenge, the small subset are the pooled predictions of previous systems.
We show that this closed-world evaluation significantly penalizes new system improvements.
%that make correct predictions outside the reach of previous systems.
% Using lazy than online evaluation.
%We break from this paradigm by evaluating newly predicted facts on-demand through crowdsourcing, lazily simulating an open-world.
To address this bias,
we introduce a new on-demand evaluation in which a system's predictions are immediately evaluated through crowdsourcing,
thus lazily simulating an open-world.
% TODO: Should we mention exhaustive annotation?
%Moreover, we debias estimates of recall by exhaustively annotating a few randomly sampled documents.
With additional careful sampling and reweighting,
%to reduce the cost of evaluation and variance of precision and recall decreases with each additional system evaluated.
% PL: don't need to talk about variance, since can convert variance reduction into cost reduction
we are able to produce scores at a fraction of the cost on a mock evaluation of the 2016 TAC-KBP challenge.

\plg{this is a sentence more for the conclusion}
We hope that the combination of a cost-effective automated evaluation with a quick turnaround time along with diverse exhaustive annotations will drive progress in knowledge base population and information extraction as a whole.

%Unfortunately, we find that fixing these problems by constructing a larger ``gold-standard'' dataset is infeasible, costing at least \todo{US\$1,000,000} by our estimates.
%Instead, we propose and implement a new online evaluation methodology that removes the problem of pooling bias and is able to provide statistically valid results cost effectively through judicious sampling of system submissions and crowdsourcing.
%A mock evaluation conducted on the 2015 KBP submissions confirms that we are able to provide statistically significant results at about \$100/system.
%% ARUN: This is getting really wordy.
%The methodology will be made available as an online evaluation platform that is easy to submit to.
%%We have implemented this methodology through an online evaluation platform
%%\todo{sell the benefits of submitting to this challenge?}.
%We hope this new platform will enable quick progress in the field.
%\pl{too long}


%We propose opening the closed world for knowledge base populatoin by introducing an online evaluation platform that immediately assesses newly predicted facts through crowdsourcing.
