Many information extraction tasks have long been evaluated using a closed-world assumption. 
Under this assumption, facts outside the “gold” annotation set are assumed to be untrue, largely due to the prohibitive cost of exhaustively annotating a sufficiently large corpus.
We analyze data from the TAC-KBP challenge, an annual competition for knowledge base population, and empirically show that this closed world assumption can penalize novel improvements in systems because their predictions are falsely marked negative.
We propose opening the closed world for knowledge base populatoin by introducing an online evaluation platform that immediately assesses newly predicted facts through crowdsourcing.
Moreover, we ensure that estimates are unbiased by exhaustively annotating a few randomly sampled documents.
Through careful sampling, the cost of evaluation and variance of scores decreases with each additional system evaluated.
We are able to obtain scores at a fraction of the cost of and with half the variance of official scores on the 2016 TAC-KBP task through a mock evaluation.
We hope that the combination of a cost-effective automated evaluation with a quick turnaround time along with diverse exhaustive annotations will drive progress in knowledge base population and information extraction as a whole.
%Unfortunately, we find that fixing these problems by constructing a larger ``gold-standard'' dataset is infeasible, costing at least \todo{US\$1,000,000} by our estimates.
%Instead, we propose and implement a new online evaluation methodology that removes the problem of pooling bias and is able to provide statistically valid results cost effectively through judicious sampling of system submissions and crowdsourcing.
%A mock evaluation conducted on the 2015 KBP submissions confirms that we are able to provide statistically significant results at about \$100/system.
%% ARUN: This is getting really wordy.
%The methodology will be made available as an online evaluation platform that is easy to submit to.
%%We have implemented this methodology through an online evaluation platform
%%\todo{sell the benefits of submitting to this challenge?}.
%We hope this new platform will enable quick progress in the field.
%\pl{too long}
