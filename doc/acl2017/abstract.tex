Despite significant efforts made over the last 10 years in the areas of entity linking, relation extraction and event extraction automated knowledge base populations (KBP) systems continue to fair abysmally against human annotators.
We analyze submissions to the TAC-KBP challenge over the last 3 years and find the current dataset-driven evaluation methodology to be inadequate in confirming measured improvements to statistical validity due its small size.
Even more alarmingly, we find the evaluation protocol difficult to use during development because it is inherently biased against systems that produce new extractions. % because of incomplete labelling in the dataset.
Unfortunately, tackling these problems through the construction of a larger ``gold-standard'' dataset is infeasible costing at least \todo{US\$1,000,000} by our estimates.
Instead, we propose a new online evaluation methodology that ameliorates the problem of pooling bias and provides statistically valid results through judicious labelling of system submissions with crowdsourcing, for under \$100 a submission.
We have implemented this methodology through an online evaluation platform \todo{sell the benefits of submitting to this challenge}.
We hope this new platform will enable quick progress in the field.
