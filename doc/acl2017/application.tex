\section{Applying on-demand open-world evaluation to KBP}
% TODO: include specific details to evaluating on KBP.

%There are two key problems to address in on-demand open-world evaluation.
%%There are two further problems we must address. %three inter-related challenges that we must tackle.
%First, we must ensure that our estimates of recall accurately reflect open world recall despite us not knowing the complete set of true relations.
%% being evaluating on an evaluation set constructed from system predictions.
%% We achieve this by calibrating our scores on a small set of exhaustively annotated documents.
%Secondly, we must ensure that the relations we sample allow us to compute unbiased precision scores.
%%entity-level and relation-level precision scores.
%%  We achieve this through structured sampling.
%%$Finally, we would to implement our evaluation framework cost-effectively.
%%  We achieve this by combining annotations across systems and sampling schemes through importance reweighing.
%We discuss our solutions to these problems next.

%On the other hand, it is much easier to verify a relation instance predicted by a system.
% TODO: outline the structure of the framework better.
% TODO: this should mention pooled annotations, etc.


\subsection{Ensuring open-world recall with exhaustive annotations}
% A line about pooled recall
\pl{I think we should use more formal notation from before to be more precise}

The pooled collection of evaluated relations in the evaluation set is unlikely to be an accurate estimate for all the true relations in the corpus and thus the pooled recall would be a biased estimate.
At the same time, it is impossible to know how many true relations there are in the corpus without annotating it in its entirety.
We address this problem by obtaining exhaustive annotations on a few documents to correct for the bias in using pooled recall.
% vv is possibly too much detail.
%For each sampled document, a crowdworker annotates any entities he/she can find.
%In a separate task, crowdworkers are then asked to identify relations between every pair of mentions in a single sentence.
%%\footnote{%
%%While there certainly are relations across sentences, this requires crowdworkers to evaluate many more mention pairs for relations.}
%During entity annotation, crowdworkers also link entities to Wikipedia pages if possible, allowing us to perform entity-level evaluation.

%We first evaluate whether exhaustive annotations can be reliably performed by crowdworkers.
%To do so, we compare crowdsourced annotations against those of expert annotators using data from the TAC-KBP 2015 EDL task~\citep{}.\footnote{%
%Further details regarding the annotation interface and experiment can be found in \refsec{turk}.
%}
%We find that crowdworkers identify 90\% of the entity spans identified by expert annotators and have significant token-level inter-annotator agreement (\fake{$\kappa = 0.8$}), validating the hypothesis that exhaustive annotation crowdsourced.

Then, when evaluating recall, we first estimate recall of the entire evaluation set on the exhaustively annotated documents, and multiply that by the recall of the system on the evaluation set. Through this two-step process we exploit the fact that it is easier to reduce variance by increasing the size of the evaluation set than it is to increase the size of the exhaustive annotation. \refsec{power} provides more details about this variance reduction.

\pl{Sounds too much like an ad-hoc side note;
this should be part of the problem definition (desired evaluation metric that we're trying to estimate),
not part of our solution; in other words, should be moved to a previous section
}
A final issue to discuss is how documents should be sampled to capture diverse entities that span documents. % to provide better entity-level scores.
When considering uniformly sampled documents, we that find that only extremely frequent entities like the United States or Barack Obama appear across documents.
Unfortunately, we do not know which entities are present in which documents to construct a fairer sampling scheme.
As a heuristic, the 20\% of our exhaustive document collection is sampled uniformly and annotated.
We then uniformly sample the entities annotated to create a collection of ``query entities''.
Finally, we construct the remaining 80\% of our document collection by searching for documents that contain the query entities according to an exact string match.
% TODO: would be nice to quantify 

\subsection{Structured sampling for entity-level and relation-level precision}
Estimation of relation-level and entity-level scores respectively assumes that relation types and entities are sampled uniformly.
However, if we were to uniformly sample relations from a typical system's predictions, 
  we would get a very skewed distribution over relation types and entities and thus bias our estimates.
We address this problem through two structured sampling schemes.

First, for relation-level scores, we stratify the predicted instances by relation and collect the same number of samples from each relation.
Secondly, for entity-level scores,  we first sample subject entities uniformly from the predicted KB, then sample fills for that entity, followed by instances for the fill.
For each such entity, we draw several samples.
% TODO: show a diagram.

Similarly, the distribution over relation instances for each system and scoring scheme (i.e.\ entity-level and relation-level) will be quite different. We correct for bias through importance reweighing~\citep{}.
