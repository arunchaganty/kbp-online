\section{Related Work}
\label{sec:related}

% Axes of comparison:
% - identifying and measuring pooling bias
% Origin of pooling: K. Sp¨ arck Jones and C. J. van Rijsbergen. Report on the need for and provision of
%an ‘ideal’ test collection. Technical report, University Computer Laboratory, Cam- bridge, 1975
The subject of pooling bias has been extensively studied in the information retrieval community.
\citep{zobel1998reliable} was the first to examine the effects of pooling bias on the TREC AdHoc task, but 
both \citet{zobel1998reliable} and \citet{voorhees199overview} concluded that the effects of pooling bias were not significant in information retrieval evaluation.
However, \citet{buckley2007bias} later identified that the small effect of pooling bias can be explained because all the leading submissions were very similar in methods-- they found that a novel system they had proposed would have experienced a 23\% point drop in scores were it not part of the pool!
We adapt the leave-one-out methodology to measure pooling bias from \citep{zobel1998reliable}.
An excellent survey of literature pertaining to pooling bias in information retrieval evaluation can be found in \citet{weber2010measurement}.

% - solutions to pooling bias 
Likewise, many solutions to the pooling bias problem have been studied in the context of information retrieval,
  from modified scoring metrics that exploit the ranking-based view of relevance~\cite{buckley2004incomplete},
  to adaptive sampling schemes that increase the amount of data that can be collected for the same budget~\cite{}.
Few of these ideas carry over to the KBP setting: 
  performance in KBP is not easily reflected by a pairwise ranking judgement as it is in the relevance setting. It is also arguable if this is an approach one wants to take. 
  The pooling bias persists in KBP evaluations despite the fact that \textit{all answers} reported by systems for a given set of entities are evaluated for correctness; the only way this could be changed is if systems are no longer required to produce a unique evidence for a extracted relation-- approximated by anydoc, our results show that it is still insufficient.

The sampling based approach we've taken to estimating performance measures is very similar to~\cite{aslam2006statistical}, where the authors derive an appropriate sampling distribution for several popular measures including mean-average precision. The paper also approximates an optimal sampling distribution based on the set of submissions; in our setting, we do not have the set of all submissions before hand.

While pooling bias hasn't been studied on KBP before, \cite{surdeanau} does study the effect of incomplete labels on distant supervision.

% - crowdsourcing kbp and nlp
Several authors have successfully used crowdsourcing for information extraction, e.g.\ for semantic role labeling~\cite{zettlemoyer}, building semantic databases~\cite{} and most recently creating a knowledge base for gun-violence related events~\cite{pavlick}.
While we also use crowdsourcing in our experiments, our focus is on \textit{evaluating systems}, not necessarily collecting an exhaustive dataset. As a result, we are able to integrate the systems we are evaluating into our data collection process.

% - online evaluation methodology?


%Incomplete: Buckley and Voorhees 2004
%Zobel 1998 -- identifies pooling bias.
%estimated 50 - 70\% of documents found by pooling.
%Zobel 1998 and Voorhees and Harman 1999 -- concluded not significant
%Only looked at systems within the submissions which were similar.
%Buckley et al 2007 repeated experiments on Robust track and found a 23\% drop!



% \paragraph{Related work.}
% 
% Cite a bunch of IR evaluation related work on estimation, pooling bias and variance reduction. Weber's thesis for score standardization and pooling bias.
% 
% Cite ACE tasks -- KBP system papers for evaluation methodology. 
% 
% Cite Ellie Pavlick's gun violence database -- also say that KBP is a whole-body workout for NLP -- main distinction is that we care about rigorous evaluation of submissions -- set up for helping development -- plan to do event extraction at a later point in time.
% David's paper on ~WordNet.
% 
% Cite Dan Klein's paper on statistical significance for NLP.
% Cite TACRED.

% KBP evaluation papers

% Cite paper 

\section{Discussion}
\label{sec:discussion}

% %(1 page)
% 
% 
% \paragraph{Discussion.}
% 
% We started this work by challenging what was needed to improve on KBP.
% Identified the difficulty of evaluating development as a cause.
% Online platform as a solution.
% 
% Some caveats of this platform -- the scores are not stable --  statistically within confidence interval, thus scores should not change much; furthermore, we believe results should be quoted with confidence intervals anyways.
% 
% Only part of the problem.
% A large dataset is equally important.
% Presence of this platform allows us to decouple dataset creation from task. -- e.g. can use and evaluate distantly supervised datasets more easily.
